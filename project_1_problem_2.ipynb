{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## simple 2.1 (for 2.2) (remove if needed)"
      ],
      "metadata": {
        "id": "v8jp4irC57O4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpV6pELe5yfM",
        "outputId": "e0601018-cad7-4e61-f2d2-ac7988c1f7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# If you haven't already downloaded NLTK stopwords (and other corpora), run:\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def custom_preprocessor(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 dimension reduction"
      ],
      "metadata": {
        "id": "qoz3a-_g6McX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from scipy.linalg import norm\n",
        "\n",
        "# Weâ€™ll use a subset of categories for the binary classification tasks\n",
        "categories = ['comp.graphics', 'sci.med']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=('headers', 'footers', 'quotes')  # Remove metadata for cleaner text\n",
        ")\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "print(\"Number of training samples:\", len(newsgroups_train.data))\n",
        "print(\"Number of testing samples:\", len(newsgroups_test.data))\n",
        "print(\"Categories:\", newsgroups_train.target_names)\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words='english',\n",
        "    lowercase=True,\n",
        "    strip_accents='unicode',\n",
        "    min_df=2,\n",
        "    max_df=0.95\n",
        ")\n",
        "\n",
        "# Transform training data\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "\n",
        "# Transform test data using the same vectorizer\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Get target variables\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "CayGK_jx6Otj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing NMF for various r"
      ],
      "metadata": {
        "id": "Yo0ViWPN6R6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def calculate_reconstruction_error(X, W, H):\n",
        "    X_reconstructed = W.dot(H)\n",
        "    return norm(X.toarray() - X_reconstructed, 'fro')**2\n",
        "\n",
        "# requires X_train, X_test, from BOW matrices from Problem 2.1?\n",
        "\n",
        "r_values = [1, 10, 50, 100, 200, 500, 1000, 2000]\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "\n",
        "# Initialize and fit the NMF model\n",
        "for r in r_values:\n",
        "    print(f\"Analyzing NMF with {r} components\")\n",
        "    nmf = NMF(n_components=r, init='random', random_state=42)\n",
        "\n",
        "    W_train = nmf.fit_transform(X_train)\n",
        "    H = nmf.components_\n",
        "\n",
        "    # transform test data\n",
        "    W_test = nmf.transform(X_test)\n",
        "\n",
        "    # calculate reconstruction errors\n",
        "    train_error = calculate_reconstruction_error(X_train, W_train, H)\n",
        "    test_error = calculate_reconstruction_error(X_test, W_test, H)\n",
        "\n",
        "    train_errors.append(train_error)\n",
        "    test_errors.append(test_error)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(r_values, train_errors, marker='o', label='Training Error')\n",
        "plt.plot(r_values, test_errors, marker='s', label='Test Error')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Number of Topics (r)')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('NMF Reconstruction Error vs Number of Topics')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9jr09jQn6SoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing topic coherence"
      ],
      "metadata": {
        "id": "WW6V_bAD6Uti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints the top words for each topic in H.\n",
        "def print_top_words(H, feature_names, n_top_words = 10):\n",
        "    for topic_idx, topic_weights in enumerate(H):\n",
        "        top_word_indices = topic_weights.argsort()[:-n_top_words-1:-1]\n",
        "        top_words = [feature_names[i] for i in top_word_indices]\n",
        "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
        "\n",
        "# Example: analyzing topics for r = 10\n",
        "r = 10\n",
        "nmf_10 = NMF(n_components=r, init='random', random_state=42)\n",
        "W_10 = nmf_10.fit_transform(X_train)\n",
        "H_10 = nmf_10.components_\n",
        "\n",
        "# list of feature names from vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print_top_words(H_10, feature_names, n_top_words=10)\n"
      ],
      "metadata": {
        "id": "_cQdkeu16VpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}