{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I_QKZDgRgNa"
      },
      "source": [
        "#1.1 Linear Regression with Closed-Form Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4K4yKSq5EPEs"
      },
      "outputs": [],
      "source": [
        "# helper functions\n",
        "import numpy as np\n",
        "\n",
        "def add_bias(X):\n",
        "    n_samples = X.shape[0]\n",
        "    bias = np.ones((n_samples, 1))\n",
        "    X_bias = np.hstack((bias, X))\n",
        "    return X_bias\n",
        "\n",
        "def loss(X, y, weights):\n",
        "    predictions = X.dot(weights)\n",
        "    errors = y - predictions\n",
        "    mse = np.mean(errors ** 2)\n",
        "    return mse, predictions, errors\n",
        "\n",
        "\n",
        "def standardize(X_train, X_test):\n",
        "    mean = X_train.mean(axis=0)\n",
        "    std = X_train.std(axis=0)\n",
        "\n",
        "    std_replaced = np.where(std == 0, 1, std)\n",
        "    X_train_std = (X_train - mean) / std_replaced\n",
        "    X_test_std = (X_test - mean) / std_replaced\n",
        "    return X_train_std, X_test_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PtiS0-HbRf5z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# closed form solution to svd\n",
        "def svd_linear_regression(X, y):\n",
        "    # bias already added\n",
        "    X_bias = X\n",
        "\n",
        "    # performing singular value decomposition\n",
        "    U, sigma, VT = np.linalg.svd(X_bias, full_matrices = False)\n",
        "\n",
        "    # pseudo inverse of sigma\n",
        "    sigma_inv = np.zeros_like(sigma)\n",
        "\n",
        "    for i in range(len(sigma)):\n",
        "        if sigma[i] > 1e-10:\n",
        "            sigma_inv[i] = 1 / sigma[i]\n",
        "\n",
        "    Sigma_inv = np.diag(sigma_inv)\n",
        "\n",
        "    # pseudo inverse of X\n",
        "    VT_transposed = VT.T\n",
        "    U_transposed = U.T\n",
        "    # X = U * Σ * V^T\n",
        "    X_inv = VT_transposed.dot(Sigma_inv).dot(U_transposed)\n",
        "\n",
        "    # (X_inv * y) where X_inv = V * Σ^+ * U^T\n",
        "    weights = X_inv.dot(y)\n",
        "    return weights\n",
        "\n",
        "# closed form solution to ridge regression (can use either svd or this for handling numerical instability)\n",
        "def linear_regression_ridge(X, y):\n",
        "    lamd = 1e-6\n",
        "\n",
        "    X_bias = X # bias already added\n",
        "\n",
        "    X_bias_transposed = X_bias.T\n",
        "    X_bias_transposed_X = X_bias_transposed.dot(X_bias)\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    identity_matrix = np.eye(n_features)\n",
        "    identity_matrix[0,0] = 0 # to not regularize bias term\n",
        "\n",
        "    # (X^T * X + lamda * I)\n",
        "    X_bias_transposed_X_reg = X_bias_transposed_X + lamd * identity_matrix\n",
        "\n",
        "    # X^T * y\n",
        "    X_bias_transposed_y = X_bias_transposed.dot(y)\n",
        "\n",
        "    # weights (X^T * X + lamda * I)^-1 * X^T * y\n",
        "    weights = np.linalg.inv(X_bias_transposed_X_reg).dot(X_bias_transposed_y)\n",
        "    return weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QRSMWEB_QBO"
      },
      "source": [
        "# 1.2 Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "IGMvKdFH_S_0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "\n",
        "def batch_gradient_descent(X, y, num_iterations, verbose = False):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.random.randn(n_features) * 0.0001\n",
        "    loss_history = []\n",
        "    alpha = 1e-7\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # loss\n",
        "        mse, predictions, errors = loss(X, y, weights)\n",
        "        loss_history.append(mse)\n",
        "\n",
        "        if i > 0 and abs(loss_history[-2] - loss_history[-1]) < tolerance:\n",
        "            print(f\"Batch GD converged at iteration {i + 1}\")\n",
        "            break\n",
        "\n",
        "        X_tranposed = X.T\n",
        "        # ∇L(wt) = -2/n * X^T * (y - Xw)\n",
        "        gradient = (-2 / n_samples) * (X_tranposed.dot(errors))\n",
        "\n",
        "        # wt+1 = wt - α * ∇L(wt)\n",
        "        weights = weights - alpha * gradient\n",
        "\n",
        "        if verbose and (i + 1) % 1000 == 0:\n",
        "            print(f\"Iteration {i + 1}: Loss = {mse}\")\n",
        "\n",
        "    return weights, loss_history\n",
        "\n",
        "\n",
        "\n",
        "def stochastic_gradient_descent(X, y, max_epochs=100, verbose = False):\n",
        "    np.random.seed(42)\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.random.randn(n_features) * 0.0001\n",
        "    loss_history = []\n",
        "    alpha = 1e-7\n",
        "    tolerance = 1e-6\n",
        "    previous_loss = float('inf')\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        random_index = np.random.permutation(n_samples)\n",
        "        X_shuffled = X[random_index]\n",
        "        y_shuffled = y[random_index]\n",
        "\n",
        "        for j in range(n_samples):\n",
        "            xi = X_shuffled[j]\n",
        "            yi = y_shuffled[j]\n",
        "\n",
        "            mse, predictions, errors = loss(xi, yi, weights)\n",
        "\n",
        "            xi_transposed = xi.T\n",
        "            # ∇L(wt) = -2* Xi * (y - Xw)\n",
        "            gradient = (-2 * xi * errors)\n",
        "\n",
        "            # wt+1 = wt - α * ∇L(wt)\n",
        "            weights = weights - alpha * gradient\n",
        "\n",
        "            # current loss every 100 updates\n",
        "            if (epoch  * n_samples + j) % 100 == 0:\n",
        "                current_loss, predictions, errors = loss(X, y, weights)\n",
        "                loss_history.append(current_loss)\n",
        "\n",
        "                # check convergence\n",
        "                if abs(previous_loss - current_loss) < tolerance:\n",
        "                    if verbose:\n",
        "                        print(f\"SGD converged at epoch {epoch + 1}, sample {j + 1}\")\n",
        "                    return weights, loss_history\n",
        "                previous_loss = current_loss\n",
        "\n",
        "        if verbose:\n",
        "            current_loss, predictions, errors = loss(X, y, weights)\n",
        "            print(f\"SGD Epoch {epoch + 1}: Loss = {current_loss}\")\n",
        "\n",
        "    return weights, loss_history\n",
        "\n",
        "\n",
        "def mini_batch_gradient_descent(X, y, max_epochs=100, verbose=False):\n",
        "    np.random.seed(42)\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.random.randn(n_features) * 0.0001\n",
        "    loss_history = []\n",
        "    alpha = 1e-7\n",
        "    batch_size = 64\n",
        "    tolerance = 1e-6\n",
        "    previous_loss = float('inf')\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        # shuffling data\n",
        "        random_index = np.random.permutation(n_samples)\n",
        "        X_shuffled = X[random_index]\n",
        "        y_shuffled = y[random_index]\n",
        "\n",
        "        for j in range(0, n_samples, batch_size):\n",
        "            end_idx = j + batch_size\n",
        "            batch_X = X_shuffled[j:end_idx]\n",
        "            batch_y = y_shuffled[j:end_idx]\n",
        "\n",
        "            mse, predictions, errors = loss(batch_X, batch_y, weights)\n",
        "            loss_history.append(mse)\n",
        "\n",
        "            batch_X_tranposed = batch_X.T\n",
        "            # ∇L(wt) = -2/n * X^T * (y - Xw)\n",
        "            gradient = (-2 / batch_X.shape[0]) * (batch_X_tranposed.dot(errors))\n",
        "            # wt+1 = wt - α * ∇L(wt)\n",
        "            weights = weights - alpha * gradient\n",
        "\n",
        "            # check convergence\n",
        "            if abs(previous_loss - mse) < tolerance:\n",
        "                if verbose:\n",
        "                    print(f\"Mini-batch GD converged at epoch {epoch + 1}, batch {j // batch_size}\")\n",
        "                return weights, loss_history\n",
        "            previous_loss = mse\n",
        "\n",
        "        # current loss every 10 epochs\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            mse, predictions, errors = loss(batch_X, batch_y, weights)\n",
        "            print(f\"Mini-batch GD Epoch {epoch + 1}: Loss = {mse}\")\n",
        "\n",
        "    return weights, loss_history\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLDT5_umgXLb"
      },
      "source": [
        "# 1.3 Comparison with scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NztxJQ7OgZK7",
        "outputId": "25325d59-bca7-44a1-eb65-6d4bcea7c84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "Training set shape: (16512, 8)\n",
            "Testing set shape: (4128, 8)\n",
            "scikit-learn LinearRegression:\n",
            "Training MSE: 0.5179\n",
            "Testing MSE: 0.5559\n",
            "Training Time: 0.0205 seconds\n",
            "\n",
            "Closed-Form Solution (SVD):\n",
            "Training MSE: 0.5179\n",
            "Testing MSE: 0.5559\n",
            "Training Time: 0.0241 seconds\n",
            "\n",
            "Closed-Form Solution (Ridge Regression):\n",
            "Training MSE: 0.5179\n",
            "Testing MSE: 0.5559\n",
            "Training Time: 0.0013 seconds\n",
            "\n",
            "Batch GD converged at iteration 67185\n",
            "Batch Gradient Descent:\n",
            "Training MSE: 1.2512\n",
            "Testing MSE: 1.2259\n",
            "Training Time: 25.2519 seconds\n",
            "\n",
            "Stochastic Gradient Descent:\n",
            "Training MSE: 0.8022\n",
            "Testing MSE: 0.8048\n",
            "Training Time: 54.8140 seconds\n",
            "\n",
            "Mini-batch Gradient Descent:\n",
            "Training MSE: 1.2951\n",
            "Testing MSE: 1.2692\n",
            "Training Time: 1.0335 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the California Housing Dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Feature names for reference\n",
        "feature_names = housing.feature_names\n",
        "print(\"Features:\", feature_names)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Standardized data\n",
        "\"\"\"\n",
        "X_train_std, X_test_std = standardize(X_train, X_test)\n",
        "\n",
        "X_train_bias = add_bias(X_train_std)\n",
        "X_test_bias = add_bias(X_test_std)\n",
        "\"\"\"\n",
        "\n",
        "# unstandardized data (currently in use)\n",
        "X_train_bias = add_bias(X_train)\n",
        "X_test_bias = add_bias(X_test)\n",
        "\n",
        "\n",
        "def sklearn_linear_regression(X_train, y_train, X_test, y_test):\n",
        "    model = LinearRegression()\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train[:,1:], y_train)  # scikit-learn handles bias internally\n",
        "    training_time = time.time() - start_time\n",
        "    y_train_pred = model.predict(X_train[:,1:])\n",
        "    y_test_pred = model.predict(X_test[:,1:])\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    return model, train_mse, test_mse, training_time\n",
        "\n",
        "\n",
        "\n",
        "# train scikit-learn's LinearRegression\n",
        "model_sklearn, mse_train_sklearn, mse_test_sklearn, time_sklearn = sklearn_linear_regression(\n",
        "    X_train_bias,\n",
        "    y_train,\n",
        "    X_test_bias,\n",
        "    y_test\n",
        ")\n",
        "\n",
        "print(\"scikit-learn LinearRegression:\")\n",
        "print(f\"Training MSE: {mse_train_sklearn:.4f}\")\n",
        "print(f\"Testing MSE: {mse_test_sklearn:.4f}\")\n",
        "print(f\"Training Time: {time_sklearn:.4f} seconds\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "SVD and Ridge Regression\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Closed-Form Solution using SVD\n",
        "start_time = time.time()\n",
        "w_closed_svd = svd_linear_regression(X_train_bias, y_train)\n",
        "time_closed_svd = time.time() - start_time\n",
        "mse_train_closed_svd, predictions, errors = loss(X_train_bias, y_train, w_closed_svd)\n",
        "mse_test_closed_svd, predictions, errors = loss(X_test_bias, y_test, w_closed_svd)\n",
        "\n",
        "print(\"Closed-Form Solution (SVD):\")\n",
        "print(f\"Training MSE: {mse_train_closed_svd:.4f}\")\n",
        "print(f\"Testing MSE: {mse_test_closed_svd:.4f}\")\n",
        "print(f\"Training Time: {time_closed_svd:.4f} seconds\\n\")\n",
        "\n",
        "# Closed-Form Solution using Ridge Regression\n",
        "start_time = time.time()\n",
        "w_closed_ridge = linear_regression_ridge(X_train_bias, y_train)\n",
        "time_closed_ridge = time.time() - start_time\n",
        "mse_train_closed_ridge, predictions, errors = loss(X_train_bias, y_train, w_closed_ridge)\n",
        "mse_test_closed_ridge, predictions, errors = loss(X_test_bias, y_test, w_closed_ridge)\n",
        "\n",
        "print(\"Closed-Form Solution (Ridge Regression):\")\n",
        "print(f\"Training MSE: {mse_train_closed_ridge:.4f}\")\n",
        "print(f\"Testing MSE: {mse_test_closed_ridge:.4f}\")\n",
        "print(f\"Training Time: {time_closed_ridge:.4f} seconds\\n\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Batch Gradient Descent\n",
        "Stochastic Gradient Descent\n",
        "Mini-batch Gradient Descent\n",
        "\"\"\"\n",
        "\n",
        "# Batch Gradient Descent\n",
        "num_iterations = 100000\n",
        "\n",
        "start_time = time.time()\n",
        "w_batch, loss_history_batch = batch_gradient_descent(\n",
        "    X_train_bias,\n",
        "    y_train,\n",
        "    num_iterations,\n",
        "    verbose=False\n",
        ")\n",
        "time_batch = time.time() - start_time\n",
        "mse_train_batch, predictions, errors = loss(X_train_bias, y_train, w_batch)\n",
        "mse_test_batch, predictions, errors = loss(X_test_bias, y_test, w_batch)\n",
        "\n",
        "print(\"Batch Gradient Descent:\")\n",
        "print(f\"Training MSE: {mse_train_batch:.4f}\")\n",
        "print(f\"Testing MSE: {mse_test_batch:.4f}\")\n",
        "print(f\"Training Time: {time_batch:.4f} seconds\\n\")\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "start_time = time.time()\n",
        "w_sgd, loss_history_sgd = stochastic_gradient_descent(\n",
        "    X_train_bias,\n",
        "    y_train,\n",
        "    max_epochs=100,\n",
        "    verbose=False\n",
        ")\n",
        "time_sgd = time.time() - start_time\n",
        "mse_train_sgd, predictions, errors = loss(X_train_bias, y_train, w_sgd)\n",
        "mse_test_sgd, predictions, errors = loss(X_test_bias, y_test, w_sgd)\n",
        "\n",
        "print(\"Stochastic Gradient Descent:\")\n",
        "print(f\"Training MSE: {mse_train_sgd:.4f}\")\n",
        "print(f\"Testing MSE: {mse_test_sgd:.4f}\")\n",
        "print(f\"Training Time: {time_sgd:.4f} seconds\\n\")\n",
        "\n",
        "# Mini-batch Gradient Descent\n",
        "\n",
        "start_time = time.time()\n",
        "w_mini, loss_history_mini = mini_batch_gradient_descent(\n",
        "    X_train_bias,\n",
        "    y_train,\n",
        "    max_epochs=100,\n",
        "    verbose=False\n",
        ")\n",
        "time_mini = time.time() - start_time\n",
        "mse_train_mini, predictions, errors = loss(X_train_bias, y_train, w_mini)\n",
        "mse_test_mini, predictions, errors = loss(X_test_bias, y_test, w_mini)\n",
        "\n",
        "print(\"Mini-batch Gradient Descent:\")\n",
        "print(f\"Training MSE: {mse_train_mini:.4f}\")\n",
        "print(f\"Testing MSE: {mse_test_mini:.4f}\")\n",
        "print(f\"Training Time: {time_mini:.4f} seconds\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZv-QLjFjj3w"
      },
      "source": [
        "Relative Difference in Learned Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tDpdCaDTjgyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c7cd9a-72f7-4026-efd5-5085fc95aec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative Difference in Learned Parameters:\n",
            "Closed-Form Solution (SVD): 0.000000\n",
            "Closed-Form Solution (Ridge Regression): 0.000000\n",
            "Batch Gradient Descent: 0.999994\n",
            "Stochastic Gradient Descent (SGD): 0.999912\n",
            "Mini-batch Gradient Descent: 0.999998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Relative Difference in Learned Parameters\n",
        "def relative_difference(w1, w2):\n",
        "    return np.linalg.norm(w1 - w2) / np.linalg.norm(w2)\n",
        "\n",
        "# Extract scikit-learn's weights (including intercept)\n",
        "w_sklearn = np.concatenate(([model_sklearn.intercept_], model_sklearn.coef_))\n",
        "\n",
        "# Compute relative differences\n",
        "rel_diff_closed_svd = relative_difference(w_closed_svd, w_sklearn)\n",
        "rel_diff_closed_ridge = relative_difference(w_closed_ridge, w_sklearn)\n",
        "rel_diff_batch = relative_difference(w_batch, w_sklearn)\n",
        "rel_diff_sgd = relative_difference(w_sgd, w_sklearn)\n",
        "rel_diff_mini = relative_difference(w_mini, w_sklearn)\n",
        "\n",
        "print(\"Relative Difference in Learned Parameters:\")\n",
        "print(f\"Closed-Form Solution (SVD): {rel_diff_closed_svd:.6f}\")\n",
        "print(f\"Closed-Form Solution (Ridge Regression): {rel_diff_closed_ridge:.6f}\")\n",
        "print(f\"Batch Gradient Descent: {rel_diff_batch:.6f}\")\n",
        "print(f\"Stochastic Gradient Descent (SGD): {rel_diff_sgd:.6f}\")\n",
        "print(f\"Mini-batch Gradient Descent: {rel_diff_mini:.6f}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}